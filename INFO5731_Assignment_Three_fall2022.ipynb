{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahithaPoduvu/Mahitha_INFO5731_-Spring2023/blob/main/INFO5731_Assignment_Three_fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "57ec8388-554e-47cc-c2c2-b0473d4bbcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of     Unnamed: 0                                             tweets\n",
              "0            0  Since the start of the pandemic, 1,144,461 Ame...\n",
              "1            1  Masks not required for people staying in outdo...\n",
              "2            2  Since the start of the pandemic, 1,144,461 Ame...\n",
              "3            3  We were able to go from original sample to ful...\n",
              "4            4  Since the start of the pandemic, 1,144,461 Ame...\n",
              "5            5  @MarahChibwana from Malawi is a medical doctor...\n",
              "6            6  Day 1123, 26.02.23 at 8AM | India #COVID19 upd...\n",
              "7            7  #India new #COVID cases crosses 200 after 50da...\n",
              "8            8  @drayeshaverrall @nzlabour @minhealthnz @chris...\n",
              "9            9  Since the start of the pandemic, 1,144,461 Ame...\n",
              "10          10  #Males recovered from mild #COVID19 have basel...>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd # for dataframes\n",
        "import numpy as np  # for arrays\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/twitter_data.csv')\n",
        "df.head\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for preprocessing data\n",
        "# removes pattern in the input text\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for word in r:\n",
        "        input_txt = re.sub(word, \"\", input_txt)\n",
        "    return input_txt\n",
        "#data.head()\n",
        "#Lowercase all texts\n",
        "df['tweets'] = df['tweets'].str.lower()\n",
        "# remove twitter handles (@user)\n",
        "df['clean_tweets'] = np.vectorize(remove_pattern)(df['tweets'], \"@[\\w]*\")\n",
        "# remove special characters, numbers and punctuations\n",
        "df['clean_tweets'] = df['clean_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "# remove short words less tha 3 characters long\n",
        "#df['clean_tweets'] = df['clean_tweets'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "4V_bN35Kr0nN",
        "outputId": "3d705ce7-ce77-40f5-bbf8-cc94f33cb413"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-24a784d27902>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['clean_tweets'] = df['clean_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                             tweets  \\\n",
              "0           0  since the start of the pandemic, 1,144,461 ame...   \n",
              "1           1  masks not required for people staying in outdo...   \n",
              "2           2  since the start of the pandemic, 1,144,461 ame...   \n",
              "3           3  we were able to go from original sample to ful...   \n",
              "4           4  since the start of the pandemic, 1,144,461 ame...   \n",
              "\n",
              "                                        clean_tweets  \n",
              "0  since the start of the pandemic            ame...  \n",
              "1  masks not required for people staying in outdo...  \n",
              "2  since the start of the pandemic            ame...  \n",
              "3  we were able to go from original sample to ful...  \n",
              "4  since the start of the pandemic            ame...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6550986f-0be1-47ca-b276-332d04fa3991\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweets</th>\n",
              "      <th>clean_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>since the start of the pandemic, 1,144,461 ame...</td>\n",
              "      <td>since the start of the pandemic            ame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>masks not required for people staying in outdo...</td>\n",
              "      <td>masks not required for people staying in outdo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>since the start of the pandemic, 1,144,461 ame...</td>\n",
              "      <td>since the start of the pandemic            ame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>we were able to go from original sample to ful...</td>\n",
              "      <td>we were able to go from original sample to ful...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>since the start of the pandemic, 1,144,461 ame...</td>\n",
              "      <td>since the start of the pandemic            ame...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6550986f-0be1-47ca-b276-332d04fa3991')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6550986f-0be1-47ca-b276-332d04fa3991 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6550986f-0be1-47ca-b276-332d04fa3991');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#N-grams\n",
        "import collections\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "a = (df['clean_tweets'].str.count('people'))\n",
        "print(a.sum())\n",
        "trigrams = []\n",
        "#TextBlob(df['clean_tweets'][0]).ngrams(3)\n",
        "for tweet in df['clean_tweets'] :\n",
        "  trigrams += ngrams(tweet.split(), 3)\n",
        "pd.Series(trigrams).value_counts(normalize=True)\n",
        "\n",
        "bigrams = []\n",
        "for tweet in df['clean_tweets'] :\n",
        "  bigrams += ngrams(tweet.split(), 2)\n",
        "pd.Series(trigrams).value_counts(normalize=True)\n",
        "\n",
        "\n",
        "\n",
        "for element in bigrams :\n",
        "  print(element)\n",
        "  print(bigrams.count(element)/(df['clean_tweets'].str.count(element[1]).sum()))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovyNBVZzugRY",
        "outputId": "77b088b5-1a32-4001-b8aa-8c681b8cf877"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "('since', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'start')\n",
            "0.6666666666666666\n",
            "('start', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'pandemic')\n",
            "0.6666666666666666\n",
            "('pandemic', 'americans')\n",
            "1.0\n",
            "('americans', 'have')\n",
            "0.8\n",
            "('have', 'died')\n",
            "1.0\n",
            "('died', 'from')\n",
            "0.25\n",
            "('from', '#covid')\n",
            "0.3333333333333333\n",
            "('#covid', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'all')\n",
            "0.8\n",
            "('all', 'deaths')\n",
            "0.6666666666666666\n",
            "('deaths', 'worldwide')\n",
            "1.0\n",
            "('worldwide', 'that')\n",
            "1.0\n",
            "('that', 'is')\n",
            "0.3333333333333333\n",
            "('is', 'about')\n",
            "1.0\n",
            "('about', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'same')\n",
            "1.0\n",
            "('same', 'as')\n",
            "0.16666666666666666\n",
            "('as', 'alaska')\n",
            "1.0\n",
            "('alaska', 'airlines')\n",
            "1.0\n",
            "('airlines', 'flight')\n",
            "1.0\n",
            "('flight', 's')\n",
            "0.0070921985815602835\n",
            "('s', 'which')\n",
            "0.25\n",
            "('which', 'killed')\n",
            "1.0\n",
            "('killed', 'people')\n",
            "0.8\n",
            "('people', 'in')\n",
            "0.12903225806451613\n",
            "('in', 'https')\n",
            "0.36363636363636365\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'bdibip')\n",
            "1.0\n",
            "('bdibip', 'wt')\n",
            "1.0\n",
            "('masks', 'not')\n",
            "1.0\n",
            "('not', 'required')\n",
            "1.0\n",
            "('required', 'for')\n",
            "0.5\n",
            "('for', 'people')\n",
            "0.2\n",
            "('people', 'staying')\n",
            "1.0\n",
            "('staying', 'in')\n",
            "0.03225806451612903\n",
            "('in', 'outdoor')\n",
            "1.0\n",
            "('outdoor', 'areas')\n",
            "1.0\n",
            "('areas', '#macao')\n",
            "1.0\n",
            "('#macao', 'will')\n",
            "1.0\n",
            "('will', 'scrap')\n",
            "1.0\n",
            "('scrap', 'mask')\n",
            "0.25\n",
            "('mask', 'wearing')\n",
            "1.0\n",
            "('wearing', 'order')\n",
            "1.0\n",
            "('order', 'starting')\n",
            "1.0\n",
            "('starting', 'from')\n",
            "0.0625\n",
            "('from', 'february')\n",
            "1.0\n",
            "('february', 'as')\n",
            "0.041666666666666664\n",
            "('as', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', 'is')\n",
            "0.08333333333333333\n",
            "('is', 'stabilized')\n",
            "1.0\n",
            "('stabilized', 'in')\n",
            "0.03225806451612903\n",
            "('in', 'the')\n",
            "0.07142857142857142\n",
            "('the', 'city')\n",
            "0.5\n",
            "('city', 'sar')\n",
            "1.0\n",
            "('sar', 'govt')\n",
            "1.0\n",
            "('govt', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'hn')\n",
            "0.3333333333333333\n",
            "('hn', 'ztorzvt')\n",
            "1.0\n",
            "('since', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'start')\n",
            "0.6666666666666666\n",
            "('start', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'pandemic')\n",
            "0.6666666666666666\n",
            "('pandemic', 'americans')\n",
            "1.0\n",
            "('americans', 'have')\n",
            "0.8\n",
            "('have', 'died')\n",
            "1.0\n",
            "('died', 'from')\n",
            "0.25\n",
            "('from', '#covid')\n",
            "0.3333333333333333\n",
            "('#covid', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'all')\n",
            "0.8\n",
            "('all', 'deaths')\n",
            "0.6666666666666666\n",
            "('deaths', 'worldwide')\n",
            "1.0\n",
            "('worldwide', 'that')\n",
            "1.0\n",
            "('that', 'is')\n",
            "0.3333333333333333\n",
            "('is', 'about')\n",
            "1.0\n",
            "('about', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'same')\n",
            "1.0\n",
            "('same', 'as')\n",
            "0.16666666666666666\n",
            "('as', 'babbs')\n",
            "1.0\n",
            "('babbs', 'switch')\n",
            "1.0\n",
            "('switch', 'fires')\n",
            "1.0\n",
            "('fires', 'which')\n",
            "0.25\n",
            "('which', 'killed')\n",
            "1.0\n",
            "('killed', 'people')\n",
            "0.8\n",
            "('people', 'in')\n",
            "0.12903225806451613\n",
            "('in', 'https')\n",
            "0.36363636363636365\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'gxiobrim')\n",
            "1.0\n",
            "('gxiobrim', 'x')\n",
            "0.14285714285714285\n",
            "('we', 'were')\n",
            "1.0\n",
            "('were', 'able')\n",
            "1.0\n",
            "('able', 'to')\n",
            "0.1111111111111111\n",
            "('to', 'go')\n",
            "0.5\n",
            "('go', 'from')\n",
            "0.0625\n",
            "('from', 'original')\n",
            "1.0\n",
            "('original', 'sample')\n",
            "0.5\n",
            "('sample', 'to')\n",
            "0.1111111111111111\n",
            "('to', 'full')\n",
            "0.5\n",
            "('full', 'genome')\n",
            "1.0\n",
            "('genome', 'sequence')\n",
            "1.0\n",
            "('sequence', 'in')\n",
            "0.03225806451612903\n",
            "('in', 'lt')\n",
            "1.0\n",
            "('lt', 'hours')\n",
            "1.0\n",
            "('hours', 'using')\n",
            "0.5\n",
            "('using', 'ims')\n",
            "1.0\n",
            "('ims', 'pcr')\n",
            "1.0\n",
            "('pcr', 'from')\n",
            "0.0625\n",
            "('from', 'from')\n",
            "0.0625\n",
            "('from', 'on')\n",
            "0.09090909090909091\n",
            "('on', '#ont')\n",
            "1.0\n",
            "('#ont', 'technology')\n",
            "1.0\n",
            "('technology', '#nanopore')\n",
            "1.0\n",
            "('#nanopore', 'using')\n",
            "0.5\n",
            "('using', 'capacity')\n",
            "1.0\n",
            "('capacity', 'developed')\n",
            "1.0\n",
            "('developed', 'before')\n",
            "1.0\n",
            "('before', 'and')\n",
            "0.1111111111111111\n",
            "('and', 'during')\n",
            "1.0\n",
            "('during', 'the')\n",
            "0.07142857142857142\n",
            "('the', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', 'pandemic')\n",
            "0.16666666666666666\n",
            "('pandemic', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'trvjnwonwe')\n",
            "1.0\n",
            "('since', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'start')\n",
            "0.6666666666666666\n",
            "('start', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'pandemic')\n",
            "0.6666666666666666\n",
            "('pandemic', 'americans')\n",
            "1.0\n",
            "('americans', 'have')\n",
            "0.8\n",
            "('have', 'died')\n",
            "1.0\n",
            "('died', 'from')\n",
            "0.25\n",
            "('from', '#covid')\n",
            "0.3333333333333333\n",
            "('#covid', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'all')\n",
            "0.8\n",
            "('all', 'deaths')\n",
            "0.6666666666666666\n",
            "('deaths', 'worldwide')\n",
            "1.0\n",
            "('worldwide', 'that')\n",
            "1.0\n",
            "('that', 'is')\n",
            "0.3333333333333333\n",
            "('is', 'about')\n",
            "1.0\n",
            "('about', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'same')\n",
            "1.0\n",
            "('same', 'as')\n",
            "0.16666666666666666\n",
            "('as', 'cerritos')\n",
            "1.0\n",
            "('cerritos', 'mid')\n",
            "1.0\n",
            "('mid', 'air')\n",
            "0.5\n",
            "('air', 'collisions')\n",
            "1.0\n",
            "('collisions', 'which')\n",
            "0.25\n",
            "('which', 'killed')\n",
            "1.0\n",
            "('killed', 'people')\n",
            "0.8\n",
            "('people', 'in')\n",
            "0.12903225806451613\n",
            "('in', 'https')\n",
            "0.36363636363636365\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'euud')\n",
            "1.0\n",
            "('euud', 'yrff')\n",
            "1.0\n",
            "('from', 'malawi')\n",
            "1.0\n",
            "('malawi', 'is')\n",
            "0.08333333333333333\n",
            "('is', 'a')\n",
            "0.007142857142857143\n",
            "('a', 'medical')\n",
            "0.5\n",
            "('medical', 'doctor')\n",
            "1.0\n",
            "('doctor', 'by')\n",
            "0.5\n",
            "('by', 'profession')\n",
            "1.0\n",
            "('profession', 'and')\n",
            "0.1111111111111111\n",
            "('and', 'was')\n",
            "0.3333333333333333\n",
            "('was', 'working')\n",
            "1.0\n",
            "('working', 'as')\n",
            "0.041666666666666664\n",
            "('as', 'a')\n",
            "0.007142857142857143\n",
            "('a', 'junior')\n",
            "1.0\n",
            "('junior', '#clinicianscientist')\n",
            "1.0\n",
            "('#clinicianscientist', 'with')\n",
            "0.5\n",
            "('with', 'an')\n",
            "0.05555555555555555\n",
            "('an', 'interest')\n",
            "1.0\n",
            "('interest', 'in')\n",
            "0.03225806451612903\n",
            "('in', 'immunology')\n",
            "0.5\n",
            "('immunology', 'when')\n",
            "1.0\n",
            "('when', 'this')\n",
            "0.5\n",
            "('this', 'episode')\n",
            "1.0\n",
            "('episode', 'was')\n",
            "0.3333333333333333\n",
            "('was', 'released')\n",
            "1.0\n",
            "('released', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'uxjft')\n",
            "1.0\n",
            "('uxjft', 'xw')\n",
            "1.0\n",
            "('xw', '#careers')\n",
            "1.0\n",
            "('#careers', '#career')\n",
            "0.5\n",
            "('#career', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', '#pandemic')\n",
            "1.0\n",
            "('#pandemic', '#immunology')\n",
            "1.0\n",
            "('#immunology', '#biomedicalscience')\n",
            "1.0\n",
            "('#biomedicalscience', '#scientists')\n",
            "1.0\n",
            "('day', 'at')\n",
            "0.05555555555555555\n",
            "('at', 'am')\n",
            "0.06666666666666667\n",
            "('am', 'india')\n",
            "0.5\n",
            "('india', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', 'update')\n",
            "1.0\n",
            "('update', 'as')\n",
            "0.041666666666666664\n",
            "('as', 'per')\n",
            "1.0\n",
            "('per', 'moh')\n",
            "1.0\n",
            "('moh', 'active')\n",
            "0.3333333333333333\n",
            "('active', 'cases')\n",
            "0.3333333333333333\n",
            "('cases', 'new')\n",
            "0.6\n",
            "('new', 'cases')\n",
            "0.3333333333333333\n",
            "('cases', 'new')\n",
            "0.6\n",
            "('new', 'recovered')\n",
            "0.5\n",
            "('recovered', 'new')\n",
            "0.2\n",
            "('new', 'deaths')\n",
            "0.16666666666666666\n",
            "('deaths', 'from')\n",
            "0.0625\n",
            "('from', 'sk')\n",
            "0.16666666666666666\n",
            "('sk', 'bd')\n",
            "0.5\n",
            "('bd', 'from')\n",
            "0.0625\n",
            "('from', 'kl')\n",
            "0.25\n",
            "('kl', 'samples')\n",
            "1.0\n",
            "('samples', 'tested')\n",
            "1.0\n",
            "('tested', 'tpr')\n",
            "0.5\n",
            "('tpr', 'weekly')\n",
            "1.0\n",
            "('weekly', 'explore')\n",
            "1.0\n",
            "('explore', 'more')\n",
            "1.0\n",
            "('more', 'insights')\n",
            "1.0\n",
            "('insights', 'at')\n",
            "0.05555555555555555\n",
            "('at', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'ircytgbyh')\n",
            "1.0\n",
            "('ircytgbyh', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'pyh')\n",
            "1.0\n",
            "('pyh', 'ohnel')\n",
            "1.0\n",
            "('#india', 'new')\n",
            "0.2\n",
            "('new', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', 'cases')\n",
            "0.16666666666666666\n",
            "('cases', 'crosses')\n",
            "1.0\n",
            "('crosses', 'after')\n",
            "1.0\n",
            "('after', 'days')\n",
            "1.0\n",
            "('days', 'states')\n",
            "0.3333333333333333\n",
            "('states', 'report')\n",
            "1.0\n",
            "('report', 'cases')\n",
            "0.16666666666666666\n",
            "('cases', 'yesterday')\n",
            "1.0\n",
            "('yesterday', 'amp')\n",
            "0.16666666666666666\n",
            "('amp', 'states')\n",
            "0.3333333333333333\n",
            "('states', 'with')\n",
            "0.5\n",
            "('with', 'active')\n",
            "0.3333333333333333\n",
            "('active', 'cases')\n",
            "0.3333333333333333\n",
            "('cases', 'new')\n",
            "0.6\n",
            "('new', 'cases')\n",
            "0.3333333333333333\n",
            "('cases', 'deaths')\n",
            "0.16666666666666666\n",
            "('deaths', 'backlogs')\n",
            "1.0\n",
            "('backlogs', 'from')\n",
            "0.0625\n",
            "('from', '#kl')\n",
            "1.0\n",
            "('#kl', 'amp')\n",
            "0.16666666666666666\n",
            "('amp', 'from')\n",
            "0.0625\n",
            "('from', 'sikkim')\n",
            "1.0\n",
            "('sikkim', 'today')\n",
            "1.0\n",
            "('today', 'recovery')\n",
            "1.0\n",
            "('recovery', 'tests')\n",
            "1.0\n",
            "('tests', 'active')\n",
            "0.3333333333333333\n",
            "('active', 'tpr')\n",
            "0.5\n",
            "('tpr', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', '#omicron')\n",
            "1.0\n",
            "('#omicron', '#maskup')\n",
            "1.0\n",
            "('#maskup', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'umyzyaxiyl')\n",
            "1.0\n",
            "('i', 'start')\n",
            "0.16666666666666666\n",
            "('start', 'semester')\n",
            "1.0\n",
            "('semester', 'tomorrow')\n",
            "1.0\n",
            "('tomorrow', 'in')\n",
            "0.03225806451612903\n",
            "('in', 'full')\n",
            "0.5\n",
            "('full', 'body')\n",
            "1.0\n",
            "('body', 'pain')\n",
            "1.0\n",
            "('pain', 'w')\n",
            "0.024390243902439025\n",
            "('w', 'brain')\n",
            "1.0\n",
            "('brain', 'fog')\n",
            "1.0\n",
            "('fog', 'amp')\n",
            "0.16666666666666666\n",
            "('amp', 'severe')\n",
            "1.0\n",
            "('severe', 'exhaustion')\n",
            "1.0\n",
            "('exhaustion', 'from')\n",
            "0.0625\n",
            "('from', '#longcovid')\n",
            "1.0\n",
            "('#longcovid', 'we')\n",
            "0.16666666666666666\n",
            "('we', 'avoided')\n",
            "1.0\n",
            "('avoided', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', 'until')\n",
            "1.0\n",
            "('until', 'you')\n",
            "1.0\n",
            "('you', 'dropped')\n",
            "1.0\n",
            "('dropped', 'mask')\n",
            "0.25\n",
            "('mask', 'mandates')\n",
            "1.0\n",
            "('mandates', 'from')\n",
            "0.0625\n",
            "('from', 'schools')\n",
            "1.0\n",
            "('schools', 'amp')\n",
            "0.16666666666666666\n",
            "('amp', 'my')\n",
            "0.5\n",
            "('my', 'son')\n",
            "1.0\n",
            "('son', 'was')\n",
            "0.3333333333333333\n",
            "('was', 'sent')\n",
            "1.0\n",
            "('sent', 'home')\n",
            "1.0\n",
            "('home', 'w')\n",
            "0.024390243902439025\n",
            "('w', 'it')\n",
            "0.14285714285714285\n",
            "('it', 'if')\n",
            "1.0\n",
            "('if', 'i')\n",
            "0.007142857142857143\n",
            "('i', 'can')\n",
            "0.2\n",
            "('can', 't')\n",
            "0.006802721088435374\n",
            "('t', 'manage')\n",
            "1.0\n",
            "('manage', 'at')\n",
            "0.05555555555555555\n",
            "('at', 'uni')\n",
            "0.5\n",
            "('uni', 'this')\n",
            "0.5\n",
            "('since', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'start')\n",
            "0.6666666666666666\n",
            "('start', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'pandemic')\n",
            "0.6666666666666666\n",
            "('pandemic', 'americans')\n",
            "1.0\n",
            "('americans', 'have')\n",
            "0.8\n",
            "('have', 'died')\n",
            "1.0\n",
            "('died', 'from')\n",
            "0.25\n",
            "('from', '#covid')\n",
            "0.3333333333333333\n",
            "('#covid', 'of')\n",
            "0.4444444444444444\n",
            "('of', 'all')\n",
            "0.8\n",
            "('all', 'deaths')\n",
            "0.6666666666666666\n",
            "('deaths', 'worldwide')\n",
            "1.0\n",
            "('worldwide', 'that')\n",
            "1.0\n",
            "('that', 'is')\n",
            "0.3333333333333333\n",
            "('is', 'about')\n",
            "1.0\n",
            "('about', 'the')\n",
            "0.2857142857142857\n",
            "('the', 'same')\n",
            "1.0\n",
            "('same', 'as')\n",
            "0.16666666666666666\n",
            "('as', 'eden')\n",
            "1.0\n",
            "('eden', 'train')\n",
            "1.0\n",
            "('train', 'wrecks')\n",
            "1.0\n",
            "('wrecks', 'which')\n",
            "0.25\n",
            "('which', 'killed')\n",
            "1.0\n",
            "('killed', 'people')\n",
            "0.8\n",
            "('people', 'in')\n",
            "0.12903225806451613\n",
            "('in', 'https')\n",
            "0.36363636363636365\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'zz')\n",
            "1.0\n",
            "('zz', 'fcrqlw')\n",
            "1.0\n",
            "('#males', 'recovered')\n",
            "0.5\n",
            "('recovered', 'from')\n",
            "0.0625\n",
            "('from', 'mild')\n",
            "1.0\n",
            "('mild', '#covid')\n",
            "0.08333333333333333\n",
            "('#covid', 'have')\n",
            "0.2\n",
            "('have', 'baseline')\n",
            "1.0\n",
            "('baseline', 'immune')\n",
            "1.0\n",
            "('immune', 'states')\n",
            "0.3333333333333333\n",
            "('states', 'primed')\n",
            "1.0\n",
            "('primed', 'to')\n",
            "0.1111111111111111\n",
            "('to', 'mount')\n",
            "1.0\n",
            "('mount', 'stronger')\n",
            "1.0\n",
            "('stronger', 'responses')\n",
            "1.0\n",
            "('responses', 'to')\n",
            "0.1111111111111111\n",
            "('to', 'future')\n",
            "1.0\n",
            "('future', 'challenges')\n",
            "1.0\n",
            "('challenges', 'https')\n",
            "0.09090909090909091\n",
            "('https', 't')\n",
            "0.07482993197278912\n",
            "('t', 'co')\n",
            "0.39285714285714285\n",
            "('co', 'iuuz')\n",
            "1.0\n",
            "('iuuz', 'jlsnc')\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qyy4oeiKvaiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpKEyN4Vsx2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(20 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f-oZf7aqrhH"
      },
      "source": [
        "# **Question 3: Create your own word embedding model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSlfLGoaqrhJ"
      },
      "source": [
        "(20 points). Use the data you collected for assignment two to build a word embedding model: \n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyJ2weJMqrhK"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 4: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "\n",
        "# Link: \n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}