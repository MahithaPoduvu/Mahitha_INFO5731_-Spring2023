{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahithaPoduvu/Mahitha_INFO5731_-Spring2023/blob/main/In_class_exercise_03_02282023_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmi3tsgr5dtn"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtOYmocN5dts"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX3UQHl75dtt"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgw6p3ha5dtu"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "The text classification task chosen here is Sentiment analysis Tweets on Avatar movie. For this task, we want to build a machine learning model that can classify tweets as either positive or negative based on the sentiment expressed in the text.\n",
        "Some features that might be useful for this task include:\n",
        "\n",
        "1.Bag of words features: This represents the text as a set of words without any regard to the order of words in the text. Counting the frequency of each word in a tweet and using this as a feature for the model. This can capture important keywords that are associated with positive or negative sentiment.\n",
        "\n",
        "2.Part-of-speech features: Analyzing the syntactic structure of the text by identifying the part of speech of each word like noun, verb, adjective and using those tags as features. \n",
        "\n",
        "3.Sentiment lexicons: Using pre-built lists of words that are annotated with their sentiment polarity (positive, negative, or neutral) and counting the frequency of those words in the text. This can capture the use of words that are strongly associated with positive or negative sentiment.\n",
        "\n",
        "4.N-grams: Analyzing sequences of words of length n (e.g. bigrams, trigrams) and using those sequences as features. This can capture more complex patterns of language.\n",
        "\n",
        "5.Emotion features: To capture the emotional content in the text such as anger, fear, joy, sadness etc. These features can be useful for sentiment analysis as different emotions may indicate different sentiment.\n",
        "\n",
        "6.words embedding: Used to capture the context and meaning of the words in the text.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape   # since it is installing every time i had made it to comment\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import snscrape.modules.twitter as twitterScrapper\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "tweet_data = [] #creating the emppty list\n",
        "number = 100   # for scrapping 100 tweets\n",
        "username = input() # to prompt user input , here I have scraped @officialavatar tweets\n",
        "d = f\" from:{username}\" # defining d as f from user name and to use it in loop\n",
        "\n",
        "for i, tweets in enumerate(sntwitter.TwitterSearchScraper(d).get_items()):\n",
        "  if i > number:\n",
        "    break\n",
        "  tweet_data.append({tweets.rawContent,tweets.user.username})\n",
        "  print(tweet_data)\n",
        "\n",
        "# to create the data frame\n",
        "df = pd.DataFrame(tweet_data,columns=['tweets','username'])\n",
        "print(df)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df.to_csv(\"//content//drive//MyDrive//tweet_data.csv\")\n",
        "data = pd.read_csv(\"//content//drive//MyDrive//tweet_data.csv\")\n",
        "print(data)"
      ],
      "metadata": {
        "id": "dw0hiMQrAOlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orlq26EU5dtx"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-gPhbW95dty"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"//content//drive//MyDrive//tweet_data.csv\")\n",
        "print(data.head)\n",
        "data.info # to get the info of the data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libaries\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import nltk\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# for preprocessing data\n",
        "# removes pattern in the input text\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for word in r:\n",
        "        input_txt = re.sub(word, \"\", input_txt)\n",
        "    return input_txt\n",
        "#data.head()\n",
        "#Lowercase all texts\n",
        "data['tweets'] = data['tweets'].str.lower()\n",
        "data.head\n",
        "# remove twitter handles (@user)\n",
        "data['clean_tweets'] = np.vectorize(remove_pattern)(data['tweets'], \"@[\\w]*\")\n",
        "#data.head\n",
        "# remove special characters, numbers and punctuations\n",
        "data['clean_tweets'] = data['clean_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "#data.head()\n",
        "# remove short words less tha 3 characters long\n",
        "data['clean_tweets'] = data['clean_tweets'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
        "#data.head()\n"
      ],
      "metadata": {
        "id": "uHu5P1ow_B-C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment lexicon\n",
        "#!pip install TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "def getTextSubjectivity(tokenized_tweet):\n",
        "  return TextBlob(tokenized_tweet).sentiment.subjectivity\n",
        "#to get text polarity\n",
        "def getTextPolarity(tokenized_tweet):\n",
        "  return TextBlob(tokenized_tweet).sentiment.polarity\n",
        "\n",
        "data_subjectivity = data['clean_tweets'].apply(getTextSubjectivity)\n",
        "data_polarity = data['clean_tweets'].apply(getTextSubjectivity)\n",
        "#print(data_subjectvity,data_polarity)\n",
        "\n",
        "def getTextAnalysis(a):\n",
        "  if a<0:\n",
        "    return \"Negative\"\n",
        "  elif a==0:\n",
        "    return \"Neutral\"\n",
        "  else:\n",
        "    return \"Positive\"\n",
        "data_score= data_polarity.apply(getTextAnalysis)\n",
        "\n",
        "# to create the data frame\n",
        "frame = { 'Tweets': data['tweets'], 'Polarity': data_polarity, 'Score' : data_score, 'Subjectivity' : data_subjectivity }\n",
        "#Creating DataFrame by passing Dictionary\n",
        "df = pd.DataFrame(frame)\n",
        "#print(df.head)\n",
        "\n",
        "#to get the percentage of positive tweets\n",
        "positive = df[df['Score']==\"Positive\"]\n",
        "print(str(positive.shape[0]/(df.shape[0])*100)+\"% of positive tweets\")\n",
        "pos =positive.shape[0]/df.shape[0]*100\n",
        "\n",
        "#to get the percentage of negative tweets\n",
        "negative = df[df['Score']==\"Negative\"]\n",
        "print(str(negative.shape[0]/(df.shape[0])*100)+\"% of negative tweets\")\n",
        "pos = negative.shape[0]/df.shape[0]*100\n",
        "\n",
        "#to get the percentage of neutral tweets\n",
        "neutral = df[df['Score']==\"Neutral\"]\n",
        "print(str(neutral.shape[0]/(df.shape[0])*100)+\"% of neutral tweets\")\n",
        "pos = neutral.shape[0]/df.shape[0]*100\n",
        "\n",
        "# plot bar graph \n",
        "labels=df.groupby('Score').count().index.values\n",
        "values = df.groupby('Score').size().values\n",
        "#plt.bar(labels,values)\n"
      ],
      "metadata": {
        "id": "83Apj2Ymi7hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(data['clean_tweets'])\n",
        "vectorizer.get_feature_names_out()\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "OU3dEf6swijg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#N-grams\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "TextBlob(data['clean_tweets'][0]).ngrams(2)"
      ],
      "metadata": {
        "id": "kuN-UB6Vw-uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parts of speech tagging\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for i in df.index:\n",
        "  doc = nlp(df.at[i,'Tweets'])\n",
        "  count = doc.count_by(spacy.attrs.POS)\n",
        "  for k,j in count.items():\n",
        "    print(doc.vocab[k].text, \"|\",j)\n",
        "\n",
        "  tagged = pos_tag(word_tokenize(df.at[0,'Tweets']))\n",
        "\n",
        "  #Extract all parts of speech from any text\n",
        "  chunker = RegexpParser(\"\"\"\n",
        "                    NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
        "                    P: {<IN>}            #To extract Prepositions\n",
        "                    V: {<V.*>}           #To extract Verbs\n",
        "                    PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
        "                    VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
        "                    \"\"\")\n",
        "\n",
        "  # Print all parts of speech in above sentence\n",
        "  output = chunker.parse(tagged)\n",
        "  print(output)\n",
        "  displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})"
      ],
      "metadata": {
        "id": "l7-JPDtSsQrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#words embedding\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors # load the Stanford GloVe model\n",
        "glove_input_file = '/content/gdrive/My Drive/glove.6B.100d.txt'\n",
        "word2vec_output_file = '/content/gdrive/My Drive/glove.6B.100d.txt.word2vec'\n",
        "glove_model = glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "\n",
        "filename = '/content/gdrive/My Drive/glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "model['go']\n",
        "model['away']\n",
        "(model['go'] + model['away'])/2"
      ],
      "metadata": {
        "id": "M4KbO8DGuBL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHJaFY7M5dtz"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oWBk1lL5dt0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The most important feature extracted in above data is sentiment lexicon as it gives the true emotion of tweets(positive, negative,netural)\n",
        "ranking the features in descending order: \n",
        "1.sentiment Lexicon\n",
        "2.words embedding\n",
        "3.parts of speech tagging\n",
        "4.N-grams\n",
        "5.bag of words\n",
        "# Since I didnot get access to the article mentioned above I made above statements by considering the materials on websites.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AOliJ2C5dt1"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ5yWloc5dt6"
      },
      "outputs": [],
      "source": [
        "#Finding the similarity between tweets\n",
        "!pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = df['Tweets']\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "sentence_embeddings.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(\n",
        "    [sentence_embeddings[0]],\n",
        "    sentence_embeddings[1:]\n",
        ")"
      ],
      "metadata": {
        "id": "gxb1KMaahF5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel # to initialise model and tokenizer\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "sentences = df['Tweets']\n",
        "\n",
        "tokens = {'input_ids': [], 'attention_mask': []}\n",
        "for sentence in sentences:\n",
        "    # encode each sentence and append to dictionary\n",
        "    new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
        "                                       truncation=True, padding='max_length',\n",
        "                                       return_tensors='pt')\n",
        "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "# reformat list of tensors into single tensor\n",
        "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "outputs = model(**tokens)\n",
        "outputs.keys()\n",
        "\n",
        "embeddings = outputs.last_hidden_state\n",
        "embeddings\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "9aGwih7CpG98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = tokens['attention_mask']\n",
        "attention_mask.shape\n",
        "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "mask.shape\n",
        "mask\n"
      ],
      "metadata": {
        "id": "vxaojq6rt8KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_embeddings = embeddings * mask\n",
        "masked_embeddings.shape\n",
        "masked_embeddings"
      ],
      "metadata": {
        "id": "BB-QVgwDuPr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summed = torch.sum(masked_embeddings, 1)\n",
        "summed.shape\n",
        "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "summed_mask.shape\n",
        "summed_mask\n",
        "mean_pooled = summed / summed_mask\n",
        "mean_pooled"
      ],
      "metadata": {
        "id": "j1vN4w00upqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as we get dense vector, cosine simialrity between tweets \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "mean_pooled = mean_pooled.detach().numpy()\n",
        "\n",
        "# calculate\n",
        "cosine_similarity(\n",
        "    [mean_pooled[0]],\n",
        "    mean_pooled[1:]\n",
        ")"
      ],
      "metadata": {
        "id": "-zjzSJUHvT4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}