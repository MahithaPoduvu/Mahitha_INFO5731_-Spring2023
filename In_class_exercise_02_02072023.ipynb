{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahithaPoduvu/Mahitha_INFO5731_-Spring2023/blob/main/In_class_exercise_02_02072023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNWOwvCpD2-M"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is35LyL_D2-R"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtLa5q3D2-S"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu2f8YwRD2-S"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "\n",
        "''' \n",
        "The interesting question I have in mind is research on IMBD website.\n",
        "\n",
        "what kind of data should be collected to answer the question:\n",
        "The follwing data needs to be collected:\n",
        "1.title of Movie\n",
        "2.description of Movie\n",
        "3.genra(type of content)\n",
        "4.Movie Release date and year\n",
        "5.Ratings, votings\n",
        "6.Total watch time of movie\n",
        "7.Gross income of movie\n",
        "8.Top movies per year/month\n",
        "9.Cast of movie\n",
        "10.Details of production crew\n",
        "11.Metascore\n",
        "12.Details about the TV series and episode\n",
        "\n",
        "\n",
        "How many data needed for analysis?\n",
        "A number of 1000 movies is needed for robustic analysis of data.\n",
        "\n",
        " steps for data collections:\n",
        " 1.Finding the relevant website platform\n",
        " 2.Inspecting and getting url\n",
        " 3.link the url to request (if we get the response as 200 then website is allowing as to scrape)\n",
        " 4.Find the tag deatils and class details of data to scrape\n",
        " 5.writing the code to extract the all tags\n",
        " 6.run the code and extract information\n",
        " 7.clean and pre process data\n",
        " 8.store the data in usage formate like excels, dataframes.\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaRdJjp7D2-U"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnGwIca7D2-U"
      },
      "outputs": [],
      "source": [
        "# installing libraries\n",
        "#!pip install lxml\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "headers ={\"Accept-Language\": \"en-US,en;q=0.5\"} # to get everything in US english with factorial 0.5\n",
        "\n",
        "#creating empty list\n",
        "movie_title=[]\n",
        "release_year=[]\n",
        "rating=[]\n",
        "time=[]\n",
        "metascore = []\n",
        "gross_profit=[]\n",
        "votes=[]\n",
        "\n",
        "\n",
        "allpages =np.arange(1,1000,100) # to get all pages from 1 to 1000 with 100 in each.\n",
        "\n",
        "#creating loop for run all pages\n",
        "for page in allpages:\n",
        "  page = requests.get(\"https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&start=\"+str(page)+\"&ref_=adv_nxt\")\n",
        "  soup = BeautifulSoup(page.text,'html.parser')\n",
        "  movies_data = soup.find_all('div',attrs={'class':'lister-item mode-advanced'})\n",
        "  sleep(randint(4,10))\n",
        "\n",
        "  for store in movies_data:\n",
        "\n",
        "    title = store.h3.a.text\n",
        "    movie_title.append(title)\n",
        "\n",
        "    year_release = store.h3.find('span',class_ = 'lister-item-year text-muted unbold').text\n",
        "    release_year.append(year_release)\n",
        "\n",
        "    rate = store.find('div',class_ = 'inline-block ratings-imdb-rating').text\n",
        "    rating.append(rate)\n",
        "\n",
        "    runtime = store.p.find('span',class_='runtime').text\n",
        "    time.append(runtime)\n",
        "\n",
        "    meta = store.find('span',class_ = 'metascore').text if store.find('span',class_ = 'metascore') else '###'\n",
        "    metascore.append(meta)\n",
        "\n",
        "    value = store.find('span',attrs={'name':\"nv\"})\n",
        "    votes.append(value)\n",
        "\n",
        "    gross = store.find('span',attrs={'name':\"nv\"}).text \n",
        "    gross_profit.append(gross)\n",
        "\n",
        "# Using pandas creating data frame\n",
        "list_best_movies = pd.DataFrame({\"Movie title\":movie_title,\"Year\":release_year,\"Rating\":rating,\"total run time\":time,\"Metascore\":metascore,\"voting\":votes,\"Gross\":gross_profit})\n",
        "list_best_movies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ_5nf_RD2-V"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN4J3cTKD2-W"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddKYMGrxD2-V"
      },
      "outputs": [],
      "source": [
        "#installing libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from time import sleep\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
        "\n",
        "allpages =np.arange(0,990,10) # to get all pages from 0 to 990 with 10 in each.\n",
        "# creating list\n",
        "article_titles=[]\n",
        "article_abstract=[]\n",
        "article_year=[]\n",
        "article_venue=[]\n",
        "article_author=[]\n",
        "\n",
        "\n",
        "#creating loop for run all pages\n",
        "for page in allpages:\n",
        "  page = ('https://scholar.google.com/scholar?start='+str(page)+'str&q=%22information+retrieval%22&hl=en&as_sdt=0,44&as_ylo=2012&as_yhi=2022')\n",
        "  response = requests.get(page,headers=headers)\n",
        "  soup = BeautifulSoup(response.content, 'lxml')\n",
        "  sleep(randint(4,10))\n",
        "  for item in soup.select('[data-lid]'):\n",
        "    title = item.select('h3')[0].get_text() # to pull article titles\n",
        "    article_titles.append(title)\n",
        "    abstract = item.select('.gs_rs')[0].get_text() # to pull abstracts\n",
        "    article_abstract.append(abstract)\n",
        "    authors = item.select('.gs_a')[0].findChildren(\"a\" , recursive=False) #to pull authors\n",
        "    print(item.select('.gs_a')[0].getText)\n",
        "    author = None\n",
        "    for child in authors:\n",
        "        if author is None:\n",
        "            author = child.get_text()\n",
        "        else :\n",
        "            author = author+\",\"+child.get_text()\n",
        "    article_author.append(author)\n",
        "\n",
        "    venue = item.select('.gs_a')[0].get_text() #to pull venue (in venue we get authors, year,venue)\n",
        "    article_venue.append(venue)\n",
        "\n",
        "articles_list = pd.DataFrame({\"article title\": article_titles,\"article abstract\": article_abstract, \"article venue\": article_venue,\"article author\": article_author})\n",
        "print(articles_list)\n",
        "# this is the message I got while scraping the google scholar.\n",
        "'''\n",
        "#b'<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\\n<html>\\n<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"><meta name=\"viewport\" content=\"initial-scale=1\"><title>https://scholar.google.com/scholar?start=0str&amp;q=%22information+retrieval%22&amp;hl=en&amp;as_sdt=0,44&amp;as_ylo=2012&amp;as_yhi=2022</title></head>\\n<body style=\"font-family: arial, sans-serif; background-color: #fff; color: #000; padding:20px; font-size:18px;\" onload=\"e=document.getElementById(\\'captcha\\');if(e){e.focus();} if(solveSimpleChallenge) {solveSimpleChallenge(,);}\">\\n<div style=\"max-width:400px;\">\\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\"><br>\\n<form id=\"captcha-form\" action=\"index\" method=\"post\">\\n<noscript>\\n<div style=\"font-size:13px;\">\\n  In order to continue, please enable javascript on your web browser.\\n</div>\\n</noscript>\\n<script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\\n<script>var submitCallback = function(response) {document.getElementById(\\'captcha-form\\').submit();};</script>\\n<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LfwuyUTAAAAAOAmoS0fdqijC2PbbdH4kjq62Y1b\" data-callback=\"submitCallback\" data-s=\"4nMS10rgRL7fmAW8WfHNlFiJmpsyEGLop3dojICC_xKtBfexg02vxCwGRUJYjXm-1fgkrOt5WlPvXUjP_xTgctcL8m1ko-lxOqmf58mdJUxgTXHRuTaCB6C-9nzS_OKE1Eq3n3WzZUvNmTCUlCYtz941hFJYs3IUrcpYkPpRTDIirFpOxL0UAshVHXux4xNXeQ6p7pn65eMMvVcbaEQHH-haMeolNjRKVvuXjZxHNURLzrBPUNdly62dSOK5xEa92bPkysdv5_j3chTJgm-WAIit\"></div>\\n\\n<input type=\\'hidden\\' name=\\'q\\' value=\\'EgQiFwQkGMax8J8GIizqDRhTcO_2L9msMykcqlOOYyLN8YuIHM9x2u48KJ03xtT_zUswPRL9K48FoDIBcg\\'><input type=\"hidden\" name=\"continue\" value=\"https://scholar.google.com/scholar?start=0str&amp;q=%22information+retrieval%22&amp;hl=en&amp;as_sdt=0,44&amp;as_ylo=2012&amp;as_yhi=2022\">\\n</form>\\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\">\\n\\n<div style=\"font-size:13px;\">\\n<b>About this page</b><br><br>\\n\\nOur systems have detected unusual traffic from your computer network.  This page checks to see if it&#39;s really you sending the requests, and not a robot.  <a href=\"#\" onclick=\"document.getElementById(\\'infoDiv\\').style.display=\\'block\\';\">Why did this happen?</a><br><br>\\n\\n<div id=\"infoDiv\" style=\"display:none; background-color:#eee; padding:10px; margin:0 0 15px 0; line-height:1.4em;\">\\nThis page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the <a href=\"//www.google.com/policies/terms/\">Terms of Service</a>. The block will expire shortly after those requests stop.  In the meantime, solving the above CAPTCHA will let you continue to use our services.<br><br>This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests.  If you share your network connection, ask your administrator for help &mdash; a different computer using the same IP address may be responsible.  <a href=\"//support.google.com/websearch/answer/86640\">Learn more</a><br><br>Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly.\\n</div>\\n\\nIP address: 34.23.4.36<br>Time: 2023-02-27T02:43:18Z<br>URL: https://scholar.google.com/scholar?start=0str&amp;q=%22information+retrieval%22&amp;hl=en&amp;as_sdt=0,44&amp;as_ylo=2012&amp;as_yhi=2022<br>\\n</div>\\n</div>\\n</body>\\n</html>\\n'\n",
        "#b'<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\\n<html>\\n<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"><meta name=\"viewport\" content=\"initial-scale=1\"><title>https://scholar.google.com/scholar?start=10str&amp;q=%22information+retrieval%22&amp;hl=en&amp;as_sdt=0,44&amp;as_ylo=2012&amp;as_yhi=2022</title></head>\\n<body style=\"font-family: arial, sans-serif; background-color: #fff; color: #000; padding:20px; font-size:18px;\" onload=\"e=document.getElementById(\\'captcha\\');if(e){e.focus();} if(solveSimpleChallenge) {solveSimpleChallenge(,);}\">\\n<div style=\"max-width:400px;\">\\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\"><br>\\n<form id=\"captcha-form\" action=\"index\" method=\"post\">\\n<noscript>\\n<div style=\"font-size:13px;\">\\n  In order to continue, please enable javascript on your web browser.\\n</div>\\n</noscript>\\n<script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\\n<script>var submitCallback = function(response) {document.getElementById(\\'captcha-form\\').submit();};</script>\\n<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LfwuyUTAAAAAOAmoS0fdqijC2PbbdH4kjq62Y1b\" data-callback=\"submitCallback\" data-s=\"jNex4alCqvHQX-YBr2bzwOopiS1KTn_vBcHNuQ55JVxUCvcBl5qm4wAys9mNypgvnsA2GtoSSj1Sb1TWbWEKv6mLNQupb2PIkO0OyYL3W17LBoohJDnEuGnDRYEnNBmeyMBtaV3_PH_jSY1-lnwLjuIIxcrYTyNdYZJfcIOVJ6WQwFLbbVvhX90IfEQGMOuTVc1wryBMDZjUibQOq9vkxXcbf6QvCLitNH5L63w91MBQrT_tVX5fiK-1XEBHjMYQbsg9hqpnIshJYtPr-aF-9SwS\"></div>\\n\\n<input type=\\'hidden\\' name=\\'q\\' value=\\'EgQiFwQkGMux8J8GIiyCOfwrKdxh15PNx9rmeI-__Ir28bu2EE1O0J3fD58DoyuV9H619mcLvHuKljIBcg\\'><input type=\"hidden\" name=\"continue\" value=\"https://scholar.google.com/scholar?start=10str&amp;q=%22information+retrieval%22&amp;hl=en&amp;as_sdt=0,44&amp;as_ylo=2012&amp;as_yhi=2022\">\\n</form>\\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\">\\n\\n<div style=\"font-size:13px;\">\\n<b>About this page</b><br><br>\\n\\nOur systems have detected unusual traffic from your computer network.  This page checks to see if it&#39;s really you sending the requests, and not a robot.  <a href=\"#\" onclick=\"document.getElementById(\\'infoDiv\\').style.display=\\'block\\';\">Why did this happen?</a><br><br>\\n\\n<div id=\"infoDiv\" style=\"display:none; background-color:#eee; padding:10px; margin:0 0 15px 0; line-height:1.4em;\">\\nThis page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the <a href=\"//www.google.com/policies/terms/\">Terms of Service</a>. The block will expire shortly after those requests stop.  In the meantime, solving the above CAPTCHA will let you continue to use our services.<br><br>This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests.  If you share your network connection, ask your administrator for help &mdash; a different computer using the same IP address may be responsible.  <a href=\"//support.google.com/websearch/answer/86640\">Learn more</a><br><br>Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly.\\n</div>\\n\\nIP address: 34.23.4.36<br>Time: 2023-02-27T02:43:23Z<br>URL: https://scholar.google.com/scholar?start=10str&amp;q=%22information+retrieval%22&amp;hl=en&amp;as_sdt=0,44&amp;as_ylo=2012&amp;as_yhi=2022<br>\\n</div>\\n</div>\\n</body>\\n</html>\\n'      \n",
        "'''           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OErvztiZD2-W"
      },
      "outputs": [],
      "source": [
        "# # to scape twitter data the following libraries are needed and pandas for dataframe\n",
        "#!pip install snscrape   # since it is installing every time i had made it to comment\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import snscrape.modules.twitter as twitterScrapper\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "tweet_data = [] #creating the emppty list\n",
        "number = 1000   # for scrapping 1000 tweets\n",
        "username = input() # to prompt user input , here I have scraped @PMahomes tweets\n",
        "d = f\" from:{username}\" # defining d as f from user name and to use it in loop\n",
        "\n",
        "for i, tweets in enumerate(sntwitter.TwitterSearchScraper(d).get_items()):\n",
        "  if i > number:\n",
        "    break\n",
        "  tweet_data.append({tweets.date,tweets.rawContent,tweets.user.username,tweets.url})\n",
        "  print(tweet_data)\n",
        "\n",
        "# to create the data frame\n",
        "df = pd.DataFrame(tweet_data,columns=['datetime','tweets','username','url'])\n",
        "print(df)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}