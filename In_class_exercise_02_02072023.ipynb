{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahithaPoduvu/Mahitha_INFO5731_-Spring2023/blob/main/In_class_exercise_02_02072023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FyAOXBqGSiE"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFVvLeEIGSiJ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv20zYGOGSiL"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_0aKpG0GSiM"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "''' \n",
        "The interesting question I have in mind is research on you tube channel.\n",
        "\n",
        "what kind of data should be collected to answer the question:\n",
        "The follwing data needs to be collected:\n",
        "1.title of video\n",
        "2.description of video\n",
        "3.genra(type of content)\n",
        "4.creation date of video and uploaded date of video\n",
        "5.likes, dislikes, number of comments and views\n",
        "6.audience demographics information like name, age, gender,location.\n",
        "7.lenght of video\n",
        "8.watch time of video per day/week/month/year\n",
        "9.Subcribers count\n",
        "10.purchase activity\n",
        "11.Top videos\n",
        "12.Impression click through rate(Number of times people clicked the video)\n",
        "13.unique viewers(Same user watched on different device)\n",
        "14.Average view duration\n",
        "15.link of channel to social media site like instagram, twitter, facebook\n",
        "\n",
        "\n",
        "\n",
        "How many data needed for analysis?\n",
        "A number of 1000 videos is needed for robustic analysis of data.\n",
        " steps for data collections:\n",
        " 1.Finding the relevant video platform\n",
        " 2.Inspecting and getting url\n",
        " 3.using web scrapping tools or using API platform\n",
        " 4.writing the code\n",
        " 5.run the code and extract information\n",
        " 6.clean and pre process data\n",
        " 7.store the data in usage formate like excels, dataframes.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsKPxZabGSiN"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##installing packages\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "import time\n",
        "import json\n",
        " \n",
        "option = Options()\n",
        "option.headless = False\n",
        "\n",
        "driver= webdriver.Firefox(option=option)\n",
        "driver.implicitly_wait =8\n",
        "url=https://www.youtube.com/results?search_query=screen+junkies+honest+trailers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTJly8S0OxHC",
        "outputId": "163c08bb-8715-430a-f9d6-dded1156224a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: AutoScraper in /usr/local/lib/python3.8/dist-packages (1.1.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from AutoScraper) (2.25.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.8/dist-packages (from AutoScraper) (0.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from AutoScraper) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from bs4->AutoScraper) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->AutoScraper) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->AutoScraper) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->AutoScraper) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->AutoScraper) (4.0.0)\n",
            "['5,847', '1,358', '(26% off)', '2,469', '(10% off)', 'Save 5% more with Subscribe & Save', '2,977', '(25% off)', '2,417', '(20% off)', '607', '(23% off)', '(30% off)', '430', '7,638', '(28% off)', '14,613', '(35% off)', '2', '4,608', '(50% off)', 'Get it by Thursday, February 16', '286', '(18% off)', '8', '(33% off)', '4,275', '65,757', '(29% off)', '16,624', '(3% off)', '8,374', '(14% off)', 'Get it by Friday, February 17', '403', '₹525', '₹279', '₹375', '₹539', '₹599', '₹300', '₹399', '₹558', '₹698', '₹229', '₹299', '₹358', '₹500', '₹553', '₹850', '₹649', '₹999', '₹149', '₹369', '₹450', '₹199', '₹521', '₹745', '₹247', '₹349', '₹204', '₹210', '₹634', '₹735', '₹269', 'Lakme 9 to 5 Primer + Matte Perfect Cover Liquid Foundation, W240 Warm Beige, Natural Matte Finish - Long Lasting Full Coverage Face Makeup, 25 ml', 'SUGAR Cosmetics - Contour De Force - Mini Blush - 02 Pink Pinnacle (Deep Rose Blush) - Long Lasting, Lightweight Makeup Blusher for Face', '+28', '+3', '+13', 'LAKMÉ Lip Color Red Letter (Matte)', 'LAKMÉ Absolute Skin Natural Mousse, Ivory Fair 01, 25g', \"Large Capacity Cosmetic Travel Bag, Women's Makeup Travel Bag Portable Leather Cosmetics Bag, Makeup Storage Bags with Handle and Divider, Wide Opening Cosmetic/Makeup Organizer Bags (White)\", 'Lacto Calamine Daily Cleansing Face Wipes with Aloe Vera, Cucumber and Vitamin E, White, Pack of 2, 50 Count', 'RENEE Bollywood Filter Face Primer 15gm, Blurs Fine Lines, Wrinkles & Pores Instantly | Hydrating, Lightweight, Non-sticky', 'Lacto Calamine Light moisturising gel. Non-sticky hydrating face & body gel with niacinamide, Hyaluronic and vitamin E. For non-oily feel & glowing skin. 150g x Pack of 1', 'Just Herbs Ayurvedic Liquid Lipstick Kit Set of 5 with Long Lasting, Hydrating & Lightweight Lip Colour, Nudes & Browns - Paraben & Silicon Free', 'WOW Skin Science Onion Black Seed Hair Oil - WITH COMB APPLICATOR - Controls Hair Fall - NO Mineral Oil, Silicones, Cooking Oil & Synthetic Fragrance - 100mL', 'Lacto Calamine Face Lotion for Oil Balance - Oily Skin - 120 ml', 'Lotus Herbals Youth Rx Anti-aging Skin Care Range – Lotus Herbals Youth Rx Anti-Aging Transforming Crème – SPF 25, PA +++- 50g, As Shown Picture (LHR419050)', 'SUGAR POP Matte Mousse - 01 Berry Bar (Berry Pink) - 3.2 ml - Ultra-creamy, Rich Pigment, Water-resistant, Lightweight, Full Coverage l Lasts up to 8 to 10 hours l Liquid Lipstick for Women']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfH6AxdcGSiO"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxAWlqqZGSiO",
        "outputId": "cdc2f1ce-da47-48d1-bfa4-1c1c971c0032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "308930\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "headers = {'User-agent': 'Mozilla/5.0 (Linux; Android 5.0; SM-G920A) AppleWebKit (KHTML, like Gecko) Chrome Mobile Safari (compatible; AdsBot-Google-Mobile; +http://www.google.com/mobile/adsbot.html)'}\n",
        "\n",
        "url = 'https://dl.acm.org/action/doSearch?AllField=information'\n",
        "\n",
        "response = requests.get(url,headers=headers)\n",
        "#print(response.text)\n",
        "page_contents = response.text\n",
        "print(len(page_contents))\n",
        "soup = BeautifulSoup(response.text,'html.parser')\n",
        "print(soup)\n",
        "\n",
        "\n",
        "\n",
        "paper_tag = soup.select('hlFld-Title')\n",
        "paper_tag = soup.find_all(\"div\", {\"class\": \"hlFld-Title\"})\n",
        "print(paper_tag)\n",
        "for tag in paper_tag:\n",
        "    print(tag.select('h3')[0].get_text())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnH845DiGSiP"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Rk0RrOGSiP",
        "outputId": "0605e263-cd7d-40bf-ab61-2c9cce03ff84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.8/dist-packages (0.5.0.20230113)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from snscrape) (2.25.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from snscrape) (2022.7.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from snscrape) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "@PMahomes \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# to scape twitter data the following libraries are needed and pandas for dataframe\n",
        "#!pip install snscrape   # since it is installing every time i had made it to comment\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import snscrape.modules.twitter as twitterScrapper\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "tweet_data = [] #creating the emppty list\n",
        "number = 1000   # for scrapping 1000 tweets\n",
        "username = input() # to prompt user input , here I have scraped @PMahomes tweets\n",
        "d = f\" from:{username}\" # defining d as f from user name and to use it in loop\n",
        "\n",
        "for i, tweets in enumerate(sntwitter.TwitterSearchScraper(d).get_items()):\n",
        "  if i > number:\n",
        "    break\n",
        "  tweet_data.append({tweets.date,tweets.rawContent,tweets.user.username,tweets.url})\n",
        "  print(tweet_data)\n",
        "\n",
        "# to create the data frame\n",
        "df = pd.DataFrame(tweet_data,columns=['datetime','tweets','username','url'])\n",
        "print(df)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}