{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahithaPoduvu/Mahitha_INFO5731_-Spring2023/blob/main/INFO5731_Assignment_Four_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  to load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "df = pd.read_csv('/content/AllProductReviews.csv')\n",
        "# Remove punctuation\n",
        "df['Reviews_processed'] = \\\n",
        "df['ReviewBody'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "df['Reviews_processed'] = \\\n",
        "df['ReviewBody'].map(lambda x: x.lower())\n",
        "# Print out the first rows of papers\n",
        "df['Reviews_processed'].head()\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    textArr = text.split(' ')\n",
        "    rem_text = \" \".join([i for i in textArr if i not in stop_words])\n",
        "    return rem_text\n",
        "\n",
        "# remove stopwords from the text\n",
        "df['Reviews_processed']=df['ReviewBody'].apply(remove_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjmQhurMS0wt",
        "outputId": "51184643-2927-4136-d158-56b56822eb86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:7: DeprecationWarning: invalid escape sequence '\\.'\n",
            "<>:7: DeprecationWarning: invalid escape sequence '\\.'\n",
            "<ipython-input-11-936948bfa762>:7: DeprecationWarning: invalid escape sequence '\\.'\n",
            "  df['ReviewBody'].map(lambda x: re.sub('[,\\.!?]', '', x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n",
        "       output = []\n",
        "       for sent in texts:\n",
        "             doc = nlp(sent) \n",
        "             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
        "       return output\n",
        "text_list=df['Reviews_processed'].tolist()\n",
        "print(text_list[1])\n",
        "tokenized_reviews = lemmatization(text_list)\n",
        "print(tokenized_reviews[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbinC0S0S07j",
        "outputId": "e8d121a2-ee43-4ada-e3c9-aee8eb3abf43"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This  earphones unreliable, bought 15 days meanwhile right side ear buds got cracked automatically got divided two parts, sound quality also much good ok, one thing bass good boat earphones.Guys,Also proof attached picsPlease see think buyingâ€‹ unreliable product.Thanks.\n",
            "\n",
            "['unreliable', 'day', 'right', 'side', 'ear', 'bud', 'part', 'sound', 'quality', 'good', 'ok', 'thing', 'bass', 'good', 'boat', 'guy', 'proof', 'think', 'unreliable', 'product', 'thank']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LSA implemented with TF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000) # to play with. min_df,max_df,max_features etc...\n",
        "vect_text=vect.fit_transform(df['Reviews_processed'])\n",
        "print(vect_text.shape)\n",
        "print(vect_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0RntkmPS08_",
        "outputId": "0280c8b3-e5ae-454d-cdcd-4e354e821652"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14337, 1000)\n",
            "  (0, 937)\t0.15160674575263752\n",
            "  (0, 471)\t0.21331087511102687\n",
            "  (0, 639)\t0.18608992730799118\n",
            "  (0, 543)\t0.140830547368153\n",
            "  (0, 699)\t0.06831298870901673\n",
            "  (0, 517)\t0.14826105641721687\n",
            "  (0, 626)\t0.20294270787424512\n",
            "  (0, 986)\t0.15586554925583834\n",
            "  (0, 688)\t0.2056494221797231\n",
            "  (0, 665)\t0.18889680549599025\n",
            "  (0, 371)\t0.25003637871526385\n",
            "  (0, 525)\t0.19504046883109372\n",
            "  (0, 500)\t0.14884498382302652\n",
            "  (0, 103)\t0.15773357676412342\n",
            "  (0, 968)\t0.17558741698083066\n",
            "  (0, 124)\t0.17744037959746908\n",
            "  (0, 279)\t0.2004388324928685\n",
            "  (0, 284)\t0.12248095429427192\n",
            "  (0, 620)\t0.19360552776887544\n",
            "  (0, 703)\t0.1337088112258296\n",
            "  (0, 108)\t0.13251314891412708\n",
            "  (0, 42)\t0.23857208068813024\n",
            "  (0, 507)\t0.18308787344565752\n",
            "  (0, 902)\t0.1971588512749146\n",
            "  (0, 882)\t0.18014500713407106\n",
            "  :\t:\n",
            "  (14334, 811)\t1.0\n",
            "  (14335, 965)\t0.42196364100096095\n",
            "  (14335, 489)\t0.36476579749709415\n",
            "  (14335, 681)\t0.22202681913348815\n",
            "  (14335, 392)\t0.5709082602694996\n",
            "  (14335, 500)\t0.4332821743238647\n",
            "  (14335, 90)\t0.29854199615495175\n",
            "  (14335, 811)\t0.1918324334830812\n",
            "  (14336, 95)\t0.3186677031566635\n",
            "  (14336, 559)\t0.18247471608152602\n",
            "  (14336, 931)\t0.21633170343975366\n",
            "  (14336, 928)\t0.1803517133520953\n",
            "  (14336, 991)\t0.3010734984280199\n",
            "  (14336, 46)\t0.26926538007691386\n",
            "  (14336, 895)\t0.2604564499379018\n",
            "  (14336, 561)\t0.19052215661303123\n",
            "  (14336, 113)\t0.23881353694076596\n",
            "  (14336, 738)\t0.25072579264423234\n",
            "  (14336, 192)\t0.35339971168340634\n",
            "  (14336, 466)\t0.1979000399485818\n",
            "  (14336, 255)\t0.24572826263329536\n",
            "  (14336, 946)\t0.2202312501573046\n",
            "  (14336, 993)\t0.23999737101671045\n",
            "  (14336, 979)\t0.16025859181327964\n",
            "  (14336, 833)\t0.20597458142915873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
        "\n",
        "lsa_top=lsa_model.fit_transform(vect_text)\n",
        "print(lsa_top)\n",
        "print(lsa_top.shape)  # (no_of_doc*no_of_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_On7-vnS1K5",
        "outputId": "8bad2dc7-86fc-4116-cc5e-006304cb6e1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.13977712 -0.18783829 -0.08314075 ...  0.13937207  0.0816113\n",
            "  -0.15425938]\n",
            " [ 0.27460387 -0.11095698 -0.00549763 ... -0.02029142  0.03961626\n",
            "  -0.0769222 ]\n",
            " [ 0.28296977 -0.09249713 -0.05709184 ...  0.10714882 -0.11884324\n",
            "   0.0528003 ]\n",
            " ...\n",
            " [ 0.28927856 -0.34198113 -0.32260779 ... -0.07519257 -0.00222977\n",
            "  -0.00514724]\n",
            " [ 0.63655777  0.16367589  0.04623432 ...  0.32215024 -0.22158809\n",
            "   0.07130717]\n",
            " [ 0.05204323 -0.08497453  0.02820248 ... -0.0272668   0.14383541\n",
            "   0.20149214]]\n",
            "(14337, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=lsa_top[0]\n",
        "print(\"Document 0 :\")\n",
        "for i,topic in enumerate(l):\n",
        "  print(\"Topic \",i,\" : \",topic*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo-TXPmTUI_5",
        "outputId": "42a93fe6-ad95-474f-e60a-7e854952f353"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0 :\n",
            "Topic  0  :  13.977711778933283\n",
            "Topic  1  :  -18.78382859723754\n",
            "Topic  2  :  -8.314075240378108\n",
            "Topic  3  :  -4.539463043078819\n",
            "Topic  4  :  -10.564674461146433\n",
            "Topic  5  :  1.279148728766772\n",
            "Topic  6  :  3.556931763580249\n",
            "Topic  7  :  13.937207489502251\n",
            "Topic  8  :  8.161130211975308\n",
            "Topic  9  :  -15.425937827959165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
        "print(lsa_model.components_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo82AfphUOci",
        "outputId": "9684025c-0d04-4099-a95f-4ff2baa8336e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1000)\n",
            "[[ 8.82524963e-03  3.28404099e-03  3.32287930e-03 ...  2.02402257e-03\n",
            "   9.19135891e-04  1.70530692e-03]\n",
            " [-1.39514285e-02 -4.94069175e-03 -3.37050170e-03 ... -3.73420794e-03\n",
            "  -1.44863928e-03 -2.85931981e-03]\n",
            " [-1.86761735e-03 -2.12917519e-05 -6.57661580e-04 ...  4.62894816e-04\n",
            "  -5.62916721e-04  1.78804635e-04]\n",
            " ...\n",
            " [ 1.23764328e-02  1.01202163e-02  3.45823440e-03 ...  1.06780289e-03\n",
            "   1.10700752e-03  2.71433013e-03]\n",
            " [-8.78693210e-03 -3.32016389e-03  2.41666836e-03 ... -1.10249682e-03\n",
            "   1.36662142e-03  5.07881152e-03]\n",
            " [-3.32458787e-03 -1.89420206e-03  7.33176134e-05 ... -1.36473328e-03\n",
            "  -1.25594084e-03 -1.78289558e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KsXKLlQUOeR",
        "outputId": "603bbca5-b0c4-4e7d-8785-6aea3c3efdc8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: \n",
            "good quality sound product bass battery nice price awesome best \n",
            "\n",
            "Topic 1: \n",
            "good click dual tips concern show function switched traffic stuff \n",
            "\n",
            "Topic 2: \n",
            "product nice working money buy worst stopped amazon one months \n",
            "\n",
            "Topic 3: \n",
            "nice sound quality good base material backup superb build comfortable \n",
            "\n",
            "Topic 4: \n",
            "product quality awesome sound excellent poor material superb super build \n",
            "\n",
            "Topic 5: \n",
            "awesome bass best price earphones nice good range battery go \n",
            "\n",
            "Topic 6: \n",
            "best price range bass great excellent product earphones value money \n",
            "\n",
            "Topic 7: \n",
            "battery life backup great bass money value hours poor also \n",
            "\n",
            "Topic 8: \n",
            "money bass value great worth noise cancellation like buy waste \n",
            "\n",
            "Topic 9: \n",
            "money value worth best battery life awesome quality waste working \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.formats.format import DataFrameFormatter\n",
        "#Importing libaries\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import nltk\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "#  to load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/AllProductReviews.csv')\n",
        "df.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBQodtYVou6d",
        "outputId": "7f2111dd-e43d-426d-9de6-052c6611a4cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                             ReviewTitle  \\\n",
              "0                 Honest review of an edm music lover\\n   \n",
              "1                 Unreliable earphones with high cost\\n   \n",
              "2                            Really good and durable.\\n   \n",
              "3                     stopped working in just 14 days\\n   \n",
              "4      Just Awesome Wireless Headphone under 1000...ðŸ˜‰\\n   \n",
              "...                                                 ...   \n",
              "14332                                            Good\\n   \n",
              "14333                                 Amazing Product\\n   \n",
              "14334                                         Not bad\\n   \n",
              "14335                                  a good product\\n   \n",
              "14336           Average headphones , n overrated name\\n   \n",
              "\n",
              "                                              ReviewBody  ReviewStar  \\\n",
              "0      No doubt it has a great bass and to a great ex...           3   \n",
              "1      This  earphones are unreliable, i bought it be...           1   \n",
              "2      i bought itfor 999,I purchased it second time,...           4   \n",
              "3      Its sound quality is adorable. overall it was ...           1   \n",
              "4      Its Awesome... Good sound quality & 8-9 hrs ba...           5   \n",
              "...                                                  ...         ...   \n",
              "14332                                             Good\\n           4   \n",
              "14333             An amazing product but a bit costly.\\n           5   \n",
              "14334                                            Sound\\n           1   \n",
              "14335  the sound is good battery life is good but the...           5   \n",
              "14336  M writing this review after using for almost 7...           1   \n",
              "\n",
              "                Product  \n",
              "0      boAt Rockerz 255  \n",
              "1      boAt Rockerz 255  \n",
              "2      boAt Rockerz 255  \n",
              "3      boAt Rockerz 255  \n",
              "4      boAt Rockerz 255  \n",
              "...                 ...  \n",
              "14332        JBL T110BT  \n",
              "14333        JBL T110BT  \n",
              "14334        JBL T110BT  \n",
              "14335        JBL T110BT  \n",
              "14336        JBL T110BT  \n",
              "\n",
              "[14337 rows x 4 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for preprocessing data\n",
        "# removes pattern in the input text\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for word in r:\n",
        "        input_txt = re.sub(word, \"\", input_txt)\n",
        "    return input_txt\n",
        "#Lowercase all texts\n",
        "df['ReviewBody'] = df['ReviewBody'].str.lower()\n",
        "# remove twitter handles (@user)\n",
        "df['clean_reviews'] = np.vectorize(remove_pattern)(df['ReviewBody'], \"@[\\w]*\")\n",
        "# remove special characters, numbers and punctuations\n",
        "df['clean_reviews'] = df['clean_reviews'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "# remove short words less tha 3 characters long\n",
        "df['clean_reviews'] = df['clean_reviews'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
        "df.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS8mqm-oou7_",
        "outputId": "2d6a04c5-f2c3-403c-a95e-c0836efee37f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                             ReviewTitle  \\\n",
              "0                 Honest review of an edm music lover\\n   \n",
              "1                 Unreliable earphones with high cost\\n   \n",
              "2                            Really good and durable.\\n   \n",
              "3                     stopped working in just 14 days\\n   \n",
              "4      Just Awesome Wireless Headphone under 1000...ðŸ˜‰\\n   \n",
              "...                                                 ...   \n",
              "14332                                            Good\\n   \n",
              "14333                                 Amazing Product\\n   \n",
              "14334                                         Not bad\\n   \n",
              "14335                                  a good product\\n   \n",
              "14336           Average headphones , n overrated name\\n   \n",
              "\n",
              "                                              ReviewBody  ReviewStar  \\\n",
              "0      no doubt it has a great bass and to a great ex...           3   \n",
              "1      this  earphones are unreliable, i bought it be...           1   \n",
              "2      i bought itfor 999,i purchased it second time,...           4   \n",
              "3      its sound quality is adorable. overall it was ...           1   \n",
              "4      its awesome... good sound quality & 8-9 hrs ba...           5   \n",
              "...                                                  ...         ...   \n",
              "14332                                             good\\n           4   \n",
              "14333             an amazing product but a bit costly.\\n           5   \n",
              "14334                                            sound\\n           1   \n",
              "14335  the sound is good battery life is good but the...           5   \n",
              "14336  m writing this review after using for almost 7...           1   \n",
              "\n",
              "                Product                                      clean_reviews  \n",
              "0      boAt Rockerz 255  doubt great bass great extent noise cancellati...  \n",
              "1      boAt Rockerz 255  this earphones unreliable bought before days m...  \n",
              "2      boAt Rockerz 255  bought itfor purchased second time gifted firs...  \n",
              "3      boAt Rockerz 255  sound quality adorable overall good just weeks...  \n",
              "4      boAt Rockerz 255  awesome good sound quality battery life just w...  \n",
              "...                 ...                                                ...  \n",
              "14332        JBL T110BT                                               good  \n",
              "14333        JBL T110BT                             amazing product costly  \n",
              "14334        JBL T110BT                                              sound  \n",
              "14335        JBL T110BT  sound good battery life good wire long very go...  \n",
              "14336        JBL T110BT  writing this review after using almost months ...  \n",
              "\n",
              "[14337 rows x 5 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment lexicon\n",
        "#!pip install TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "def getTextSubjectivity(tokenized_reviews):\n",
        "  return TextBlob(tokenized_reviews).sentiment.subjectivity\n",
        "#to get text polarity\n",
        "def getTextPolarity(tokenized_tweet):\n",
        "  return TextBlob(tokenized_reviews).sentiment.polarity\n",
        "\n",
        "df_subjectivity = df['clean_reviews'].apply(getTextSubjectivity)\n",
        "df_polarity = df['clean_reviews'].apply(getTextSubjectivity)\n",
        "#print(data_subjectvity,data_polarity)\n",
        "\n",
        "def getTextAnalysis(a):\n",
        "  if a<0:\n",
        "    return \"Negative\"\n",
        "  elif a==0:\n",
        "    return \"Neutral\"\n",
        "  else:\n",
        "    return \"Positive\"\n",
        "df_score= df_polarity.apply(getTextAnalysis)"
      ],
      "metadata": {
        "id": "NdA_7iqXrr7U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to create the data frame\n",
        "frame = { 'AllProductReviews': df['ReviewBody'], 'Polarity': df_polarity, 'Score' : df_score, 'Subjectivity' : df_subjectivity }\n",
        "#Creating DataFrame by passing Dictionary\n",
        "df = pd.DataFrame(frame)\n",
        "#print(df.head)\n",
        "\n",
        "#to get the percentage of positive tweets\n",
        "positive = df[df['Score']==\"Positive\"]\n",
        "print(str(positive.shape[0]/(df.shape[0])*100)+\"% of positive tweets\")\n",
        "pos =positive.shape[0]/df.shape[0]*100\n",
        "\n",
        "#to get the percentage of negative tweets\n",
        "negative = df[df['Score']==\"Negative\"]\n",
        "print(str(negative.shape[0]/(df.shape[0])*100)+\"% of negative tweets\")\n",
        "pos = negative.shape[0]/df.shape[0]*100\n",
        "\n",
        "#to get the percentage of neutral tweets\n",
        "neutral = df[df['Score']==\"Neutral\"]\n",
        "print(str(neutral.shape[0]/(df.shape[0])*100)+\"% of neutral tweets\")\n",
        "pos = neutral.shape[0]/df.shape[0]*100\n",
        "\n",
        "# plot bar graph \n",
        "labels=df.groupby('Score').count().index.values\n",
        "values = df.groupby('Score').size().values\n",
        "#plt.bar(labels,values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhmFmRVsrr9M",
        "outputId": "08f9b7a6-ba31-43bc-be2b-29b9eeb1fdb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91.14180093464462% of positive tweets\n",
            "0.0% of negative tweets\n",
            "8.858199065355373% of neutral tweets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models , I am using different data set here.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AOb6O1J3ou_5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  to load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "data.head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG3LU9zBSsJn",
        "outputId": "a517711c-20ac-4a5c-8fbd-d38168d02031"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.fillna(0)"
      ],
      "metadata": {
        "id": "rMJkkBBo4hYZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['sentiment'] = data['sentiment'].map({'negative' : 0, 'positive' : 1})\n",
        "import string\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    Message = text.lower()\n",
        "    Message = re.sub('\\[.*?\\]', '', Message)\n",
        "    Message = re.sub('https?://\\S+|www\\.\\S+', '', Message)\n",
        "    Message = re.sub('<.*?>+', '', Message)\n",
        "    Message = re.sub('[%s]' % re.escape(string.punctuation), '', Message)\n",
        "    Message = re.sub('\\n', '', Message)\n",
        "    Message = re.sub('\\w*\\d\\w*', '', Message)\n",
        "    return Message\n",
        "\n",
        "data['cleaned_review'] = data['review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "gqj63jIalQLL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['review'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "pVtbzfyNlQNv",
        "outputId": "a7436aad-9e64-477d-ed94-cd56e655885e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned_review'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "ZOE47Jiow5Ex",
        "outputId": "0cb377dd-35f2-4c91-e860-ab42d86a10ab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one of the other reviewers has mentioned that after watching just  oz episode youll be hooked they are right as this is exactly what happened with methe first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordit is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to manyaryans muslims gangstas latinos christians italians irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awayi would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare forget pretty pictures painted for mainstream audiences forget charm forget romanceoz doesnt mess around the first episode i ever saw struck me as so nasty it was surreal i couldnt say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data[\"cleaned_review\"]\n",
        "y = data[\"sentiment\"]"
      ],
      "metadata": {
        "id": "9KEqpm0Vw5GT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
      ],
      "metadata": {
        "id": "vFBpr32AlQQr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorization = TfidfVectorizer()\n",
        "xv_train = vectorization.fit_transform(x_train)\n",
        "xv_test = vectorization.transform(x_test)\n",
        "xv_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dtezwprxdMk",
        "outputId": "105344b2-cacb-4413-dde6-87e3bab11fa9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<37500x177418 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5054856 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjs4WZvtz_kk",
        "outputId": "4a27d67a-b574-4aa6-dee1-e23fca8cff61"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review            0\n",
              "sentiment         0\n",
              "cleaned_review    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report # Evaluation for Model\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state = 123, max_depth=10, min_samples_leaf=6)\n",
        "  \n",
        "# Performing training\n",
        "dt_model.fit(xv_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "kpI5TKiVxdN6",
        "outputId": "c0f4b531-d3f2-46d6-90a1-21f450ea497f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=10, min_samples_leaf=6, random_state=123)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=10, min_samples_leaf=6, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=10, min_samples_leaf=6, random_state=123)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using gini\n",
        "y_pred = dt_model.predict(xv_test)\n",
        "\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(cf)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHOwZ6p4yjOo",
        "outputId": "d095e690-7a29-403a-89fd-a19fda04c5d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3775 2447]\n",
            " [1008 5270]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.61      0.69      6222\n",
            "           1       0.68      0.84      0.75      6278\n",
            "\n",
            "    accuracy                           0.72     12500\n",
            "   macro avg       0.74      0.72      0.72     12500\n",
            "weighted avg       0.74      0.72      0.72     12500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC  \n",
        "svc_clr = SVC(kernel='linear') \n",
        "  \n",
        "# fitting x samples and y classes \n",
        "svc_clr.fit(xv_train, y_train)"
      ],
      "metadata": {
        "id": "eF080KXHyjQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_svc = svc_clr.predict(xv_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "cf = confusion_matrix(y_test, y_pred_svc)\n",
        "classification_report = classification_report(y_test, y_pred_svc)\n",
        "\n",
        "print(cf)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "id": "vEVD33BS5HpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train , test in kf.split(df):\n",
        "  print(\"%s %s\" % (train, test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "-U4NXHwmH2KS",
        "outputId": "d4b8a391-7c07-42ba-ce3d-96217eedb527"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5c6bc3c81d05>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import numpy as np\n",
        ">>> from sklearn.model_selection import KFold\n",
        "\n",
        ">>> X = [\"a\", \"b\", \"c\", \"d\"]\n",
        ">>> kf = KFold(n_splits=2)\n",
        ">>> for train, test in kf.split(X):\n",
        "...     print(\"%s %s\" % (train, test))"
      ],
      "metadata": {
        "id": "Q3yMFmofloLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuUtJ8VSmCiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_svm = []\n",
        "score_dt = []\n",
        "for train_index,test_index in kf.split('data'):\n",
        "  x_train,x_test,y_train,y_test = data[train_index],data[test_index],\\\n",
        "                                  data.target[train_index],data.target[test_index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "5gMRB845FvNH",
        "outputId": "7b667029-dd9a-41ec-849d-4da497b913cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-54e8a11fbd51>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscore_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscore_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'kf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfvMKJjIXS5G",
        "outputId": "dcae97aa-cda5-4ba1-ecda-96c617082a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of         Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
            "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
            "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
            "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
            "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
            "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
            "\n",
            "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
            "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
            "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
            "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
            "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
            "\n",
            "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
            "0         2   2008        WD         Normal     208500  \n",
            "1         5   2007        WD         Normal     181500  \n",
            "2         9   2008        WD         Normal     223500  \n",
            "3         2   2006        WD        Abnorml     140000  \n",
            "4        12   2008        WD         Normal     250000  \n",
            "...     ...    ...       ...            ...        ...  \n",
            "1455      8   2007        WD         Normal     175000  \n",
            "1456      2   2010        WD         Normal     210000  \n",
            "1457      5   2010        WD         Normal     266500  \n",
            "1458      4   2010        WD         Normal     142125  \n",
            "1459      6   2008        WD         Normal     147500  \n",
            "\n",
            "[1460 rows x 81 columns]>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of         Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
              "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
              "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
              "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
              "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
              "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
              "\n",
              "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
              "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
              "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
              "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
              "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "\n",
              "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0         2   2008        WD         Normal     208500  \n",
              "1         5   2007        WD         Normal     181500  \n",
              "2         9   2008        WD         Normal     223500  \n",
              "3         2   2006        WD        Abnorml     140000  \n",
              "4        12   2008        WD         Normal     250000  \n",
              "...     ...    ...       ...            ...        ...  \n",
              "1455      8   2007        WD         Normal     175000  \n",
              "1456      2   2010        WD         Normal     210000  \n",
              "1457      5   2010        WD         Normal     266500  \n",
              "1458      4   2010        WD         Normal     142125  \n",
              "1459      6   2008        WD         Normal     147500  \n",
              "\n",
              "[1460 rows x 81 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "sf = pd.read_csv(\"/content/train.csv\")\n",
        "print(sf.head)\n",
        "sf.info # to get the info of the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sf['zindexvalue'] = sf['zindexvalue'].str.replace(',', '')\n",
        "sf['zindexvalue'] = sf['zindexvalue'].convert_objects(convert_numeric=True)\n",
        "sf.lastsolddate.min(), sf.lastsolddate.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "TIDaaG_87BYi",
        "outputId": "a4824b66-5788-45bf-eb76-fc942820d39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'zindexvalue'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1cea1c4deab5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zindexvalue'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zindexvalue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zindexvalue'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zindexvalue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastsolddate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastsolddate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'zindexvalue'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SH_tFu0w7Ba6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}